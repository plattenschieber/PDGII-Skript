\chapter[Iterationsverfahren für lineare Gleichungssysteme]{Iterationsverfahren für (große) lineare Gleichungssysteme}
Gegeben: $Ax=b$, $A\in \R^{n\times n}$, $A$ invertierbar, $x,b \in \R^n$ mit $x=A^{-1}b$
Ziel: Konstruktion eines iterativen Verfahren zur Lösung von * der Form:
\begin{equation}
  x^{(k+1)} := \varphi(x^{(k)}),\quad k\in \N, x^{(0)} \in \R^n
  \label{}
\end{equation}
Iterationsfunktion $\varphi: \R^n \to \R^n$ gegeben.

\section{Das Richardson-Verfahren}
Ansatz: Sei $M\in \R^{n\times n}$ invertierbar. Wir betrachten das folgende Splitting:
\begin{equation}
  Mx + \underbrace{(A-M)x}_{=N}= b
  \label{}
\end{equation}
und setzen 
\begin{equation}
  \begin{split}
    Mx^{(k+1)} + Ax^{(k)}     &= b \quad FEHLER? WO IST N?\\
    \Leftrightarrow x^{(k+1)} &= x^{(k)} + M^{-1}(b-Ax^{(k)})\\
    &= x^{(k)} + M^{-1}r^{(k)}
  \end{split}
  \label{}
\end{equation}
mit $ r^{(k)} := b-Ax^{(k)}$ das $k$-te Residuum für $k=0,1,2,\dots$

 Dieses Iterationsverfahren lässt sich durch Einführen eines Relaxationsparameters $\alpha \in \R$ noch verallgemeinern. Sei dazu $x^{(0)}\in \R^n$ gegeben. 

\begin{definition}[Richardson-Verfahren]
   \label{richardson}
   \[
     x^{(k+1)} := x^{(k)} + \alpha M^{-1} r^{(k)} \qquad k=0,1,2,\dots
   \]
   heißt (stationäres) Richardson-Verfahren. Die dazugehörige Iterationsvorschrift lautet
   \[
     \varphi(x) = Bx + c\\
   \]
   wobei $B=I-\alpha M^{-1}A$ und $C=\alpha M^{-1}b$
\end{definition}


Für die Konvergenz kennen wir aus Numerik I folgenden Satz:

\begin{satz}
  \label{itkonvergenz}
  Das Iterationsverfahren \eqref{richardson} konvergiert genau dann, wenn 
  \begin{equation}
    \rho (B) < 1 
    \label{}
  \end{equation}
  wobei $\rho(B) := \max_{\lambda \in \sigma(B)} |\lambda|$ und $\sigma(B) = \left\{ \lambda\in \C: \lambda \text{ ist EW von B} \right\}$
\end{satz}

Hinreichende Konvergenzbedingung ist $\|B\| <1$ wobei $\|\cdot \|$ eine beliebige Matrixnorm ist, die durch eine Vektornorm induziert wird.
Für den Fehler im $k$-ten Schritt $e^{(k)} = x-x^{(k)}$ gilt $\|e^{(k)}\| \leq \eta^k e^{(k)}$ mit $\eta := \|B\|$ STIMMT DIE UNGLEICHUNG?

\begin{satz}
  Das stationäre Richardson-Verfahren \eqref{richardson} mit $\alpha \neq 0$ konvergiert genau dann, wenn 
  \begin{equation}
    2* \frac{Re(\lambda_i)}{\alpha|\lambda_i|^2} > 1 \qquad \forall i=1,\dots, n
    \label{}
  \end{equation}
  wobei $\lambda_i \in \sigma(M^{-1}A)$
\end{satz}

\begin{proof}
  Wende \eqref{itkonvergenz} auf $B=I-\alpha M^{-1}A$ an. Sei $\mu_i \in \sigma(B)$. Dann gilt
  \begin{equation*}
    \begin{split}
      \mu_i = 1-\alpha \lambda_i, \quad \forall \lambda_i \in \sigma(M^{-1}A)\\
      \rho(B) < 1 \Leftrightarrow |1-\alpha\lambda_i| <1, \quad \forall \lambda_i \in \sigma(M^{-1}A)\\
      \Leftrightarrow -2 \alpha Re(\lambda_i) + \alpha^2 |\lambda_i|^2 <0
    \end{split}
  \end{equation*}
  SCHÖNERE AUSRICHTUNG MÖGLICH?
\end{proof}

\begin{satz}
  $M^{-1}A$ habe nur positive reelle Eigenwerte $\lambda_i$, $i=1,\dots, n$ (Invertierbarkeit nicht zwingend erforderlich), die wie folgt geordnet seien:
  \begin{equation*}
    0< \lambda_n \leq \lambda_{n-1} \leq \dots \leq \lambda_1
  \end{equation*}

  \begin{enumerate}[1)]
    \item  Das stationäre Richardson-Verfahren ist genau dann konvergent, wenn 
             \begin{equation*}
                0 < \alpha < \frac{2}{\lambda_1}
              \end{equation*}

    \item Der Spektralradius $\rho(B)$ ist minimal (und damit die Konvergenz am schnellsten), wenn 
      \begin{equation*}
        \alpha = \alpha_{opt} := \frac{2}{\lambda_1+\lambda_n}
      \end{equation*}
      Dann gilt
      \begin{equation*}
        \rho_{opt} = \min_{\alpha} \left( \rho(B_\alpha) \right) 
                   = \frac{\lambda_1-\lambda_n}{\lambda_1+\lambda_n} 
                   = \frac{\kappa - 1}{\kappa+1}= 1 - \frac{2}{\kappa+1}
      \end{equation*}
      mit der Konditionszahl $\kappa = \kappa(M^{-1}A) = \frac{\lambda_1}{\lambda_n}$\\
    FEHLT HIER NOCH ETWAS?
  \end{enumerate}
 \end{satz}

 \begin{proof}
   \begin{equation*}
     \mu_i \in \sigma(B) \Leftrightarrow \mu_i = 1-\alpha \lambda_i, \quad \lambda_i\in \sigma(M^{-1}A)
   \end{equation*}
   Aus Satz 1.1.1:[FÄNGT DIE NUMERIERUNG FÜR SATZ UND DEFINITION UNABHÄNGIG BEI DER SECTION AN?]\\
   Das Richardson-Verfahren konvergiert
   \begin{equation*} 
     \Leftrightarrow |\mu_i| <1, \quad \forall i = 1, \dots, n
     \Leftrightarrow |1-\alpha\lambda_i| < 1\\
     \Leftrightarrow -1 < 1 - \alpha \lambda_i < 1 
     \Leftrightarrow 0 < \alpha < \frac{2}{\lambda_i} 
     \Leftrightarrow 0 < \alpha < \frac{2}{\lambda_1} = \min_i \frac{2}{\lambda_i}
   \end{equation*}
   mit $\rho(B) = \max_i |1-\alpha\lambda_i = \max \left\{ |1-\alpha\lambda_i|, |1-\alpha\lambda_n| \right\}$
   Der kleinste Spektralradius $\rho_{opt}$ ergibt sich durch Schnittpunktbildung der beiden Geraden. Berechne den opt Schnittpunkt von $|1-\alpha\lambda_1$ und $|1-\alpha\lambda_n|$\
BILD
   \begin{equation*}
     -(1-\alpha\lambda_n) = 1-\alpha\lambda_1 
     \Leftrightarrow \alpha_{opt} = \frac{2}{\lambda_1+\lambda_n}
     \Rightarrow \rho_{opt} = 1-\alpha_{opt} \lambda_n = 1 - \frac{2}{\lambda_1+\lambda_n} \lambda_n = \frac{\lambda_1-\lambda_n}{\lambda_1+\lambda_n}
   \end{equation*}
 \end{proof}

 Die Konvergenz des stationären Richardson-Verfahrens ist also abhängig von der Konditionszahl von $M^{-1}A$ bzw. vom größten und kleinsten Eigenwert:
 \begin{equation*}
   \lambda_1 := max \lambda_i, \quad \lambda_i \in \sigma(M^{-1}A)\\
   \lambda_n := min \lambda_i
 \end{equation*}

 \begin{bemerkung}
 Die Matrix $M$ ist frei wählbar und man kann die Kondition des Systems damit a-priori beeinflussen. Daher nennt man sie auch Vorkonditionierer (engl. preconditioner) oder Vorkonditionierungsmatrix
 \begin{itemize}
   \item $M$ sollte eine gute Approximation an $A$ sein und $\kappa(M^{-1}A)$ sollte möglichst klein sein
   \item $M^{-1}$ sollte sich einfach auf einem Vektor anwenden lassen, d.h. mit vertretbarem Rechenaufwand. 
 \end{itemize}
 \end{bemerkung}

 \begin{bemerkung}
 Man spricht beim Verfahren (**) auch vom vorkonditionniertem Richardson-Verfahren ($M\neq I$). 
 Nachteil: Für optimale Konvergenz muss Information über $\lambda_1$ und $\lambda_n$ vorliegen (die Ermittlung dieser, kann mitunter so teuer wie die Berechnung des ganzen Systems sein).
 \end{bemerkung}

 \begin{algorithmus}[Vorkonditioniertes Richardson-Verfahren]
   Init: 1. Geg. $X^{(0)}\in \R^n$ (Startvektor), $\alpha\in\R \\ \{0\}$
   2. Berechne das Startresiduum $r^{(0)} := b-Ax^{(0)}$
   Iteration: while $\|r^{(k)} \leq \varepsilon \|r^{(0)}$ für gegebenes $\varepsilon$\\
   $y^{(k)} := M^{-1} r^{(k)}$\\
 \end{algorithmus}

